{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.8.5 64-bit ('image-captioning': conda)",
   "metadata": {
    "interpreter": {
     "hash": "4eeba685eeaeca2b7690177a14201459c1bc7b9dd9191f2682df96905cb14256"
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "c:\\Users\\ragha\\Desktop\\Projects\\Image-Captioning\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "os.chdir('..')\n",
    "print(os.getcwd())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Connected to GPU :  True\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pprint import pprint\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import transforms\n",
    "from torchsummary import summary\n",
    "\n",
    "from src.dataset import Flickr8k, vocab\n",
    "from src.models import Model\n",
    "from src.utils import show_imgs\n",
    "from src.train import train_fit, validation_fit\n",
    "\n",
    "print(\"Connected to GPU : \", torch.cuda.is_available())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main(params, show_imgs=False, resume_training=False):\n",
    "    \n",
    "    # Set random seeds and deterministic pytorch for reproducibility\n",
    "    torch.manual_seed(params['seed'])       # pytorch random seed\n",
    "    np.random.seed(params['seed'])          # numpy random seed\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.enabled = False\n",
    "\n",
    "    captions_csv = pd.read_csv(os.path.abspath(params['csv_filepath']))\n",
    "\n",
    "    # split the dataframe into train and testget_word_embedding\n",
    "    train, val = train_test_split(captions_csv, test_size=0.2, random_state=params['seed'])\n",
    "    # split the test set into test and validation\n",
    "    val, test = train_test_split(val, test_size=0.1, random_state=params['seed'])\n",
    "\n",
    "    train = train.iloc[:160]\n",
    "    val   = val.iloc[:40]\n",
    "\n",
    "    my_transforms_train = transforms.Compose([\n",
    "                                                #transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225]),\n",
    "                                                transforms.Resize((params['IMG_SIZE'], params['IMG_SIZE'])),\n",
    "                                                transforms.ToTensor()\n",
    "    ])\n",
    "\n",
    "    my_transforms_val = transforms.Compose([\n",
    "                                                #transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225]),\n",
    "                                                transforms.Resize((params['IMG_SIZE'], params['IMG_SIZE'])),\n",
    "                                                transforms.ToTensor()\n",
    "    ])\n",
    "\n",
    "    train_set = Flickr8k(df=train, data_dir=os.path.abspath('data'), transforms=my_transforms_train)\n",
    "    train_dataloader = DataLoader(train_set, batch_size=params['batch_size'], shuffle=True, num_workers=params['num_workers'])\n",
    "\n",
    "    val_set = Flickr8k(df=val, data_dir=os.path.abspath('data'), transforms=my_transforms_val)\n",
    "    val_dataloader = DataLoader(val_set, batch_size=params['batch_size'], shuffle=True, num_workers=params['num_workers'])\n",
    "\n",
    "    if show_imgs:\n",
    "        for i_batch, sample_batched in enumerate(train_dataloader):\n",
    "            print(i_batch, sample_batched['image'].size(), sample_batched['caption'].size())\n",
    "            # observe 4th batch and stop.\n",
    "            if i_batch == 2:\n",
    "                for idx, caption_set in enumerate(sample_batched['caption']):\n",
    "                    for caption in caption_set:\n",
    "                        for token in caption.tolist():\n",
    "                            print(token, end=\" \")\n",
    "                            #print(vocab.get_word_token(token), end=\" \")\n",
    "                        print()\n",
    "                    show_imgs(sample_batched['image'][idx])\n",
    "                break\n",
    "\n",
    "    model = Model(\n",
    "            backbone='vgg16',\n",
    "            embed_size=128, \n",
    "            hidden_size=128, \n",
    "            vocab_size=vocab.MAX_INDEX, \n",
    "            lstm_cells=128, \n",
    "            lstm_dropout=0.5,\n",
    "            verbose=False,\n",
    "            device=params['device'])\n",
    "\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=params['lr'], betas=params['betas'], eps=params['eps'], weight_decay=params['weight_decay'])\n",
    "    criterion = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "    train_loss , train_accuracy = [], []\n",
    "    val_loss , val_accuracy = [], []\n",
    "    start_epoch = 0\n",
    "\n",
    "    if resume_training:\n",
    "        loaded_checkpoint = torch.load(params['LOAD_CKPT_PATH'])\n",
    "\n",
    "        model.load_state_dict(loaded_checkpoint['state_dict'])\n",
    "        optimizer.load_state_dict(loaded_checkpoint['optimizer'])\n",
    "\n",
    "        start_epoch = loaded_checkpoint['epoch']\n",
    "\n",
    "        train_loss = loaded_checkpoint['training_loss']\n",
    "        train_accuracy = loaded_checkpoint['training_acc']\n",
    "\n",
    "        val_loss = loaded_checkpoint['val_loss']\n",
    "        val_accuracy = loaded_checkpoint['val_acc']\n",
    "\n",
    "    for epoch in range(start_epoch, params['epochs']):\n",
    "        print(f\"\\nEpoch {epoch+1} of {params['epochs']}\")\n",
    "        print(\"-\"*15)\n",
    "        print()\n",
    "\n",
    "        train_epoch_loss, train_epoch_accuracy = train_fit(params['device'], model, train_dataloader, optimizer, criterion, train_set)\n",
    "        val_epoch_loss, val_epoch_accuracy     = validation_fit(params['device'], model, val_dataloader, optimizer, criterion, val_set)\n",
    "\n",
    "        train_loss.append(train_epoch_loss)\n",
    "        train_accuracy.append(train_epoch_accuracy)\n",
    "\n",
    "        val_loss.append(val_epoch_loss)\n",
    "        val_accuracy.append(val_epoch_accuracy)\n",
    "\n",
    "        print(f\"Train Loss:\\t {train_epoch_loss:.8f}, Train Acc:\\t {train_epoch_accuracy:.8f}\")\n",
    "        print(f'Val Loss:\\t {val_epoch_loss:.8f}, Val Acc:\\t {val_epoch_accuracy:.8f}')\n",
    "\n",
    "        # save model checkpoint\n",
    "        checkpoint = {\n",
    "            'epoch'         : epoch + 1,\n",
    "            'state_dict'    : model.state_dict(),\n",
    "            'optimizer'     : optimizer.state_dict(),\n",
    "            'training_loss' : train_loss,\n",
    "            'training_acc'  : train_accuracy,\n",
    "            'val_loss'      : val_loss,\n",
    "            'val_acc'       : val_accuracy,\n",
    "        }\n",
    "\n",
    "        torch.save(checkpoint, os.path.join(os.path.abspath(params['CKPT_DIR']), 'model.pt'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "{'CKPT_DIR': 'models/',\n",
      " 'IMG_SIZE': 224,\n",
      " 'LOAD_CKPT_PATH': '',\n",
      " 'batch_size': 4,\n",
      " 'betas': (0.9, 0.999),\n",
      " 'csv_filepath': 'data/captions.csv',\n",
      " 'device': 'cuda',\n",
      " 'epochs': 2,\n",
      " 'eps': 1e-08,\n",
      " 'lr': 0.01,\n",
      " 'num_workers': 4,\n",
      " 'seed': 42,\n",
      " 'weight_decay': 0.0005}\n",
      "  0%|          | 0/40 [00:00<?, ?it/s]\n",
      "Epoch 1 of 2\n",
      "---------------\n",
      "\n",
      "100%|██████████| 40/40 [03:15<00:00,  4.89s/it]\n",
      "100%|██████████| 10/10 [00:59<00:00,  5.96s/it]\n",
      "Train Loss:\t 0.78451086, Train Acc:\t 2426.87500000\n",
      "Val Loss:\t 0.59044359, Val Acc:\t 2552.50000000\n",
      "  0%|          | 0/40 [00:00<?, ?it/s]\n",
      "Epoch 2 of 2\n",
      "---------------\n",
      "\n",
      " 42%|████▎     | 17/40 [01:07<01:20,  3.49s/it]Exception ignored in: <function _MultiProcessingDataLoaderIter.__del__ at 0x000002A72A0CF160>\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\ragha\\.conda\\envs\\image-captioning\\lib\\site-packages\\torch\\utils\\data\\dataloader.py\", line 1203, in __del__\n",
      "    self._shutdown_workers()\n",
      "  File \"C:\\Users\\ragha\\.conda\\envs\\image-captioning\\lib\\site-packages\\torch\\utils\\data\\dataloader.py\", line 1177, in _shutdown_workers\n",
      "    w.join(timeout=_utils.MP_STATUS_CHECK_INTERVAL)\n",
      "  File \"C:\\Users\\ragha\\.conda\\envs\\image-captioning\\lib\\multiprocessing\\process.py\", line 149, in join\n",
      "    res = self._popen.wait(timeout)\n",
      "  File \"C:\\Users\\ragha\\.conda\\envs\\image-captioning\\lib\\multiprocessing\\popen_spawn_win32.py\", line 108, in wait\n",
      "    res = _winapi.WaitForSingleObject(int(self._handle), msecs)\n",
      "KeyboardInterrupt: \n",
      " 42%|████▎     | 17/40 [01:11<01:36,  4.21s/it]\n"
     ]
    },
    {
     "output_type": "error",
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-4-31140d26e13b>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     18\u001b[0m     \u001b[0mpprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mparams\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     19\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 20\u001b[1;33m     main(\n\u001b[0m\u001b[0;32m     21\u001b[0m         \u001b[0mparams\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mparams\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     22\u001b[0m         \u001b[0mshow_imgs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-3-efff187c6976>\u001b[0m in \u001b[0;36mmain\u001b[1;34m(params, show_imgs, resume_training)\u001b[0m\n\u001b[0;32m     85\u001b[0m         \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     86\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 87\u001b[1;33m         \u001b[0mtrain_epoch_loss\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtrain_epoch_accuracy\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtrain_fit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mparams\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'device'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtrain_dataloader\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtrain_set\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     88\u001b[0m         \u001b[0mval_epoch_loss\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mval_epoch_accuracy\u001b[0m     \u001b[1;33m=\u001b[0m \u001b[0mvalidation_fit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mparams\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'device'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mval_dataloader\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mval_set\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     89\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\ragha\\Desktop\\Projects\\Image-Captioning\\src\\train.py\u001b[0m in \u001b[0;36mtrain_fit\u001b[1;34m(device, model, dataloader, optimizer, criterion, dataset)\u001b[0m\n\u001b[0;32m     18\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     19\u001b[0m         \u001b[1;31m# Forward pass: Pass image data from training dataset, make predictions about class image belongs to (0-4 in this case)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 20\u001b[1;33m         \u001b[0mpredicted_caption\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     21\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     22\u001b[0m         \u001b[0mpredicted_caption\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpredicted_caption\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpredicted_caption\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\.conda\\envs\\image-captioning\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m    725\u001b[0m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    726\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 727\u001b[1;33m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    728\u001b[0m         for hook in itertools.chain(\n\u001b[0;32m    729\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\ragha\\Desktop\\Projects\\Image-Captioning\\src\\models.py\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, data)\u001b[0m\n\u001b[0;32m    128\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    129\u001b[0m         \u001b[0mimg_features\u001b[0m      \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mencoder\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mimage\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 130\u001b[1;33m         \u001b[0mcaption_predicted\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdecoder\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mimg_features\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcaption\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m:\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    131\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    132\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mcaption_predicted\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\.conda\\envs\\image-captioning\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m    725\u001b[0m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    726\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 727\u001b[1;33m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    728\u001b[0m         for hook in itertools.chain(\n\u001b[0;32m    729\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\ragha\\Desktop\\Projects\\Image-Captioning\\src\\models.py\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, img_features, caption)\u001b[0m\n\u001b[0;32m     85\u001b[0m         \u001b[0membeddings\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0membed\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcaption\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     86\u001b[0m         \u001b[0membeddings\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mimg_features\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0munsqueeze\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0membeddings\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdim\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 87\u001b[1;33m         \u001b[0moutput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0m_\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlstm\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0membeddings\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     88\u001b[0m         \u001b[0moutputs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfc1\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     89\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\.conda\\envs\\image-captioning\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m    725\u001b[0m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    726\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 727\u001b[1;33m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    728\u001b[0m         for hook in itertools.chain(\n\u001b[0;32m    729\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\.conda\\envs\\image-captioning\\lib\\site-packages\\torch\\nn\\modules\\rnn.py\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, input, hx)\u001b[0m\n\u001b[0;32m    579\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcheck_forward_args\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch_sizes\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    580\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mbatch_sizes\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 581\u001b[1;33m             result = _VF.lstm(input, hx, self._flat_weights, self.bias, self.num_layers,\n\u001b[0m\u001b[0;32m    582\u001b[0m                               self.dropout, self.training, self.bidirectional, self.batch_first)\n\u001b[0;32m    583\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "if __name__ == '__main__':\n",
    "    params = {\n",
    "        'csv_filepath'  : 'data/captions.csv',\n",
    "        'CKPT_DIR'      : 'models/',\n",
    "        'LOAD_CKPT_PATH': '',\n",
    "        'seed'          : 42,\n",
    "        'device'        : 'cuda' if torch.cuda.is_available() else 'cpu',\n",
    "        'epochs'        : 2,\n",
    "        'lr'            : 0.01,\n",
    "        'betas'         : (0.9, 0.999),\n",
    "        'eps'           : 1e-8,\n",
    "        'weight_decay'  : 0.0005,\n",
    "        'num_workers'   : 4,  # simple rule 4*no.of gpu'\n",
    "        'IMG_SIZE'      : 224,\n",
    "        'batch_size'    : 4\n",
    "    }\n",
    "\n",
    "    pprint(params)\n",
    "\n",
    "    main(\n",
    "        params=params,\n",
    "        show_imgs=False,\n",
    "        resume_training=False\n",
    "        )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}